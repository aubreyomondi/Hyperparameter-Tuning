{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Parameter\n",
    "\n",
    "![model_parameter](model_parameter.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extracting a Logistic Regression parameter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a list of original variable names from the training DataFrame\n",
    "original_variables = X_train.columns\n",
    "\n",
    "# Extract the coefficients of the logistic regression estimator\n",
    "model_coefficients = log_reg_clf.coef_[0]\n",
    "\n",
    "# Create a dataframe of the variables and coefficients & print it out\n",
    "coefficient_df = pd.DataFrame({\"Variable\" : original_variables, \"Coefficient\": model_coefficients})\n",
    "print(coefficient_df.head())\n",
    "\n",
    "# Print out the top 3 positive variables\n",
    "top_three_df = coefficient_df.sort_values(by='Coefficient', axis=0, ascending=False)[0:3]\n",
    "print(top_three_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Variable   Coefficient\n",
    "\n",
    "0     LIMIT_BAL -2.886513e-06\n",
    "\n",
    "1           AGE -8.231685e-03\n",
    "\n",
    "2         PAY_0  7.508570e-04\n",
    "\n",
    "3         PAY_2  3.943751e-04\n",
    "\n",
    "4         PAY_3  3.794236e-04\n",
    "\n",
    "  Variable  Coefficient\n",
    "    \n",
    "2    PAY_0     0.000751\n",
    "\n",
    "6    PAY_5     0.000438\n",
    "\n",
    "5    PAY_4     0.000435\n",
    "\n",
    "You have succesfully extracted and reviewed a very important parameter for the Logistic Regression Model. The coefficients of the model allow you to see which variables are having a larger or smaller impact on the outcome. Additionally the sign lets you know if it is a positive or negative relationship."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extracting a Random Forest parameter\n",
    "\n",
    "You will now translate the work previously undertaken on the logistic regression model to a random forest model. A parameter of this model is, for a given tree, how it decided to split at each level.\n",
    "\n",
    "This analysis is not as useful as the coefficients of logistic regression as you will be unlikely to ever explore every split and every tree in a random forest model. However, it is a very useful exercise to peak under the hood at what the model is doing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract the 7th (index 6) tree from the random forest\n",
    "chosen_tree = rf_clf.estimators_[6]\n",
    "\n",
    "# Visualize the graph using the provided image\n",
    "imgplot = plt.imshow(tree_viz_image)\n",
    "plt.show()\n",
    "\n",
    "# Extract the parameters and level of the top (index 0) node\n",
    "split_column = chosen_tree.tree_.feature[0]\n",
    "split_column_name = X_train.columns[split_column]\n",
    "split_value = chosen_tree.tree_.threshold[0]\n",
    "\n",
    "# Print out the feature and level\n",
    "print(\"This node split on feature {}, at a value of {}\".format(split_column_name, split_value))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This node split on feature PAY_4, at a value of 1.0\n",
    "\n",
    "![tree_image](tree_image.svg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hyperparameters\n",
    "\n",
    "![hyperparameter](hyperparameter.png)\n",
    "\n",
    "![find_hyperparameters_that_matter](find_hyperparameters_that_matter.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Understanding what hyperparameters are available and the impact of different hyperparameters is a core skill for any data scientist. As models become more complex, there are many different settings you can set, but only some will have a large impact on your model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print out the old estimator, notice which hyperparameter is badly set\n",
    "print(rf_clf_old)\n",
    "\n",
    "# Get confusion matrix & accuracy for the old rf_model\n",
    "print(\"Confusion Matrix: \\n\\n {} \\n Accuracy Score: \\n\\n {}\".format(\n",
    "  \tconfusion_matrix(y_test, rf_old_predictions),\n",
    "  \taccuracy_score(y_test, rf_old_predictions)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "RandomForestClassifier(bootstrap=True, class_weight=None, criterion='gini',\n",
    "\n",
    "                max_depth=None, max_features='auto', max_leaf_nodes=None,\n",
    "                \n",
    "                min_impurity_decrease=0.0, min_impurity_split=None,\n",
    "                \n",
    "                min_samples_leaf=1, min_samples_split=2,\n",
    "                \n",
    "                min_weight_fraction_leaf=0.0, n_estimators=5, n_jobs=None,\n",
    "                \n",
    "                oob_score=False, random_state=42, verbose=0, warm_start=False)\n",
    "                \n",
    "    Confusion Matrix: \n",
    "    \n",
    "     [[276  37]\n",
    "     \n",
    "     [ 64  23]] \n",
    "     \n",
    "     Accuracy Score: \n",
    "    \n",
    "     0.7475"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Improve the above score by changing n_estimators"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a new random forest classifier with better hyperparamaters\n",
    "rf_clf_new = RandomForestClassifier(n_estimators=500)\n",
    "\n",
    "# Fit this to the data and obtain predictions\n",
    "rf_new_predictions = rf_clf_new.fit(X_train, y_train).predict(X_test)\n",
    "\n",
    "# Assess the new model (using new predictions!)\n",
    "print(\"Confusion Matrix: \\n\\n\", confusion_matrix(y_test, rf_new_predictions))\n",
    "print(\"Accuracy Score: \\n\\n\", accuracy_score(y_test, rf_new_predictions))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Confusion Matrix: \n",
    "    \n",
    "\n",
    " [[300  13]\n",
    "  \n",
    " [ 63  24]]\n",
    "\n",
    "Accuracy Score: \n",
    "\n",
    " 0.81\n",
    " \n",
    " We got a nice 5% accuracy boost just from changing the n_estimators"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build a knn estimator for each value of n_neighbours\n",
    "knn_5 = KNeighborsClassifier(n_neighbors=5)\n",
    "knn_10 = KNeighborsClassifier(n_neighbors=10)\n",
    "knn_20 = KNeighborsClassifier(n_neighbors=20)\n",
    "\n",
    "# Fit each to the training data & produce predictions\n",
    "knn_5_predictions = knn_5.fit(X_train, y_train).predict(X_test)\n",
    "knn_10_predictions = knn_10.fit(X_train, y_train).predict(X_test)\n",
    "knn_20_predictions = knn_20.fit(X_train, y_train).predict(X_test)\n",
    "\n",
    "# Get an accuracy score for each of the models\n",
    "knn_5_accuracy = accuracy_score(y_test, knn_5_predictions)\n",
    "knn_10_accuracy = accuracy_score(y_test, knn_10_predictions)\n",
    "knn_20_accuracy = accuracy_score(y_test, knn_20_predictions)\n",
    "print(\"The accuracy of 5, 10, 20 neighbours was {}, {}, {}\".format(knn_5_accuracy, knn_10_accuracy, knn_20_accuracy))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The accuracy of 5, 10, 20 neighbours was 0.7125, 0.765, 0.7825"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hyperparameter Values\n",
    "![hyperparameter_values](hyperparameter_values.png)\n",
    "\n",
    "![conflicting_hyperparameter_choices](conflicting_hyperparameter_choices.png)\n",
    "\n",
    "![silly_hyperparameter_values](silly_hyperparameter_values.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluating different hyperparamer values of single hyperparameter"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finding the best hyperparameter of interest without writing hundreds of lines of code for hundreds of models is an important efficiency gain that will greatly assist your future machine learning model building.\n",
    "\n",
    "An important hyperparameter for the GBM algorithm is the learning rate. But which learning rate is best for this problem? By writing a loop to search through a number of possibilities, collating these and viewing them you can find the best one.\n",
    "\n",
    "Possible learning rates to try include 0.001, 0.01, 0.05, 0.1, 0.2 and 0.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the learning rates & results storage\n",
    "learning_rates = [0.001, 0.01, 0.05, 0.1, 0.2, 0.5]\n",
    "results_list = []\n",
    "\n",
    "# Create the for loop to evaluate model predictions for each learning rate\n",
    "for lr in learning_rates:\n",
    "    model = GradientBoostingClassifier(learning_rate=lr)\n",
    "    predictions = model.fit(X_train, y_train).predict(X_test)\n",
    "    # Save the learning rate and accuracy score\n",
    "    results_list.append([lr, accuracy_score(y_test, predictions)])\n",
    "\n",
    "# Gather everything into a DataFrame\n",
    "results_df = pd.DataFrame(results_list, columns=['learning_rate', 'accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "learning_rate    accuracy\n",
    "\n",
    "        0.001    0.7825\n",
    "    \n",
    "        0.010    0.8025\n",
    "        \n",
    "        0.050    0.8100\n",
    "        \n",
    "        0.100    0.7975\n",
    "        \n",
    "        0.200    0.7900\n",
    "        \n",
    "        0.500    0.7775\n",
    "        \n",
    "You efficiently tested a few different values for a single hyperparameter and can easily see which learning rate value was the best. Here, it seems that a learning rate of 0.05 yields the best accuracy."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Building Learning Curves of different hyperparamer values of a single hyperparameter"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we want to test many different values for a single hyperparameter it can be difficult to easily view that in the form of a DataFrame. Previously you learned about a nice trick to analyze this. A graph called a 'learning curve' can nicely demonstrate the effect of increasing or decreasing a particular hyperparameter on the final result."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the learning rates & accuracies list\n",
    "learn_rates = np.linspace(0.01 , 2, num=30)\n",
    "accuracies = []\n",
    "\n",
    "# Create the for loop\n",
    "for learn_rate in learn_rates:\n",
    "  \t# Create the model, predictions & save the accuracies as before\n",
    "    model = GradientBoostingClassifier(learning_rate=learn_rate)\n",
    "    predictions = model.fit(X_train, y_train).predict(X_test)\n",
    "    accuracies.append(accuracy_score(y_test, predictions))\n",
    "\n",
    "# Plot results    \n",
    "plt.plot(learn_rates, accuracies)\n",
    "plt.gca().set(xlabel='learning_rate', ylabel='Accuracy', title='Accuracy for different learning_rates')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![learning_curve](learning_curve.svg)\n",
    "\n",
    "You can see that for low values, you get a pretty good accuracy. However once the learning rate pushes much above 1.5, the accuracy starts to drop. You have learned and practiced a useful skill for visualizing large amounts of results for a single hyperparameter."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Grid Search\n",
    "\n",
    "Number of models created by evaluating two hyperparameters with 5 and 10 values respectively. Even applying cross validation.\n",
    "\n",
    "![grid_search_models](grid_search_models.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Build Grid Search functions\n",
    "\n",
    "In data science it is a great idea to try building algorithms, models and processes 'from scratch' so you can really understand what is happening at a deeper level. Of course there are great packages and libraries for this work (and we will get to that very soon!) but building from scratch will give you a great edge in your data science work."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the function\n",
    "def gbm_grid_search(learn_rate, max_depth):\n",
    "\n",
    "\t# Create the model\n",
    "    model = GradientBoostingClassifier(learning_rate=learn_rate, max_depth=max_depth)\n",
    "    \n",
    "    # Use the model to make predictions\n",
    "    predictions = model.fit(X_train, y_train).predict(X_test)\n",
    "    \n",
    "    # Return the hyperparameters and score\n",
    "    return([learn_rate, max_depth, accuracy_score(y_test, predictions)])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You now have a function you can call to test different combinations of two hyperparameters for the GBM algorithm."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the relevant lists\n",
    "results_list = []\n",
    "learn_rate_list = [0.01, 0.1, 0.5]\n",
    "max_depth_list = [2, 4, 6]\n",
    "\n",
    "# Create the for loop\n",
    "for learn_rate in learn_rate_list:\n",
    "    for max_depth in max_depth_list:\n",
    "        results_list.append(gbm_grid_search(learn_rate,max_depth))\n",
    "\n",
    "# Print the results\n",
    "print(results_list) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[[0.01, 2, 0.78], [0.01, 4, 0.78], [0.01, 6, 0.76], [0.1, 2, 0.74], [0.1, 4, 0.76], [0.1, 6, 0.75], [0.5, 2, 0.73], [0.5, 4, 0.74], [0.5, 6, 0.74]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Extend the gbm_grid_search function to include the hyperparameter subsample. Name this new function gbm_grid_search_extended.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_list = []\n",
    "learn_rate_list = [0.01, 0.1, 0.5]\n",
    "max_depth_list = [2,4,6]\n",
    "\n",
    "# Extend the function input\n",
    "def gbm_grid_search_extended(learn_rate, max_depth, subsample):\n",
    "\n",
    "\t# Extend the model creation section\n",
    "    model = GradientBoostingClassifier(learning_rate=learn_rate, max_depth=max_depth, subsample=subsample)\n",
    "    \n",
    "    predictions = model.fit(X_train, y_train).predict(X_test)\n",
    "    \n",
    "    # Extend the return part\n",
    "    return([learn_rate, max_depth, subsample, accuracy_score(y_test, predictions)])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Extend your loop to call gbm_grid_search (available in your console), then test the values [0.4 , 0.6] for the subsample hyperparameter and print the results. max_depth_list & learn_rate_list are available in your environment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_list = []\n",
    "\n",
    "# Create the new list to test\n",
    "subsample_list =  [0.4 , 0.6]\n",
    "\n",
    "for learn_rate in learn_rate_list:\n",
    "    for max_depth in max_depth_list:\n",
    "    \n",
    "    \t# Extend the for loop\n",
    "        for subsample in subsample_list:\n",
    "        \t\n",
    "            # Extend the results to include the new hyperparameter\n",
    "            results_list.append(gbm_grid_search_extended(learn_rate, max_depth, subsample))\n",
    "            \n",
    "# Print results\n",
    "print(results_list)    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[[0.01, 2, 0.4, 0.73], [0.01, 2, 0.6, 0.74], [0.01, 4, 0.4, 0.73], [0.01, 4, 0.6, 0.75], [0.01, 6, 0.4, 0.72], [0.01, 6, 0.6, 0.78], [0.1, 2, 0.4, 0.74], [0.1, 2, 0.6, 0.74], [0.1, 4, 0.4, 0.73], [0.1, 4, 0.6, 0.73], [0.1, 6, 0.4, 0.74], [0.1, 6, 0.6, 0.76], [0.5, 2, 0.4, 0.64], [0.5, 2, 0.6, 0.67], [0.5, 4, 0.4, 0.72], [0.5, 4, 0.6, 0.71], [0.5, 6, 0.4, 0.63], [0.5, 6, 0.6, 0.64]]\n",
    "\n",
    "You have effectively built your own grid search! You went from 2 to 3 hyperparameters and can see how you could extend that to even more values and hyperparameters. That was a lot of effort though. Be warned - we are now entering a world that can get very computationally expensive very fast!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Grid Search\n",
    "\n",
    "![grid_search_steps](grid_search_steps.png)\n",
    "\n",
    "![grid_search_models](grid_search_models.png)\n",
    "\n",
    "![grid_search_inputs](grid_search_inputs.png)\n",
    "\n",
    "![estimator](estimator.png)\n",
    "\n",
    "![param_grid](param_grid.png)\n",
    "\n",
    "![cv](cv.png)\n",
    "\n",
    "![scoring](scoring.png)\n",
    "\n",
    "![refit](refit.png)\n",
    "\n",
    "**Refit must be set to True in order to use the best_estimator_ property of the grid search object for fitting.**\n",
    "\n",
    "![return_train_score](return_train_score.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The GridSearchCV module from Scikit Learn provides many useful features to assist with efficiently undertaking a grid search. You will now put your learning into practice by creating a GridSearchCV object with certain parameters.\n",
    "\n",
    "The desired options are:\n",
    "\n",
    "- A Random Forest Estimator, with the split criterion as 'entropy'\n",
    "- 5-fold cross validation\n",
    "- The hyperparameters max_depth (2, 4, 8, 15) and max_features ('auto' vs 'sqrt')\n",
    "- Use roc_auc to score the models\n",
    "- Use 4 cores for processing in parallel\n",
    "- Ensure you refit the best model and return training scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a Random Forest Classifier with specified criterion\n",
    "rf_class = RandomForestClassifier(criterion='entropy')\n",
    "\n",
    "# Create the parameter grid\n",
    "param_grid = {'max_depth': [2, 4, 8, 15], 'max_features': ['auto', 'sqrt']} \n",
    "\n",
    "# Create a GridSearchCV object\n",
    "grid_rf_class = GridSearchCV(\n",
    "    estimator=rf_class,\n",
    "    param_grid=param_grid,\n",
    "    scoring='roc_auc',\n",
    "    n_jobs=4,\n",
    "    cv=5,\n",
    "    refit=True, return_train_score=True)\n",
    "print(grid_rf_class)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You now understand all the inputs to a GridSearchCV object and can tune many different hyperparameters and many different values for each on a chosen algorithm!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Grid Search Output\n",
    "\n",
    "![grid_search_output](grid_search_output.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You will now explore the cv_results_ property of the GridSearchCV object defined in the video. This is a dictionary that we can read into a pandas DataFrame and contains a lot of useful information about the grid search we just undertook.\n",
    "\n",
    "A reminder of the different column types in this property:\n",
    "\n",
    "time_ columns\n",
    "param_ columns (one for each hyperparameter) and the singular params column (with all hyperparameter settings)\n",
    "a train_score column for each cv fold including the mean_train_score and std_train_score columns\n",
    "a test_score column for each cv fold including the mean_test_score and std_test_score columns\n",
    "a rank_test_score column with a number from 1 to n (number of iterations) ranking the rows based on their mean_test_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read the cv_results property into a dataframe & print it out\n",
    "cv_results_df = pd.DataFrame(grid_rf_class.cv_results_)\n",
    "print(cv_results_df)\n",
    "\n",
    "# Extract and print the column with a dictionary of hyperparameters used\n",
    "column = cv_results_df.loc[:, ['params']]\n",
    "print(column)\n",
    "\n",
    "# Extract and print the row that had the best mean test score\n",
    "best_row = cv_results_df[cv_results_df['rank_test_score'] == 1 ]\n",
    "print(best_row)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You have build invaluable skills in looking 'under the hood' at what your grid search is doing by extracting and analysing the cv_results_ property."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Analyzing the best results\n",
    "At the end of the day, we primarily care about the best performing 'square' in a grid search. Luckily Scikit Learn's gridSearchCv objects have a number of parameters that provide key information on just the best square (or row in cv_results_).\n",
    "\n",
    "Three properties you will explore are:\n",
    "\n",
    "- best_score_ – The score (here ROC_AUC) from the best-performing square.\n",
    "- best_index_ – The index of the row in cv_results_ containing information on the best-performing square.\n",
    "- best_params_ – A dictionary of the parameters that gave the best score, for example 'max_depth': 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print out the ROC_AUC score from the best-performing square\n",
    "best_score = grid_rf_class.best_score_\n",
    "print(best_score)\n",
    "\n",
    "# Create a variable from the row related to the best-performing square\n",
    "cv_results_df = pd.DataFrame(grid_rf_class.cv_results_)\n",
    "best_row = cv_results_df.loc[[grid_rf_class.best_index_]]\n",
    "print(best_row)\n",
    "\n",
    "# Get the n_estimators parameter from the best-performing square and print\n",
    "best_n_estimators = grid_rf_class.best_params_[\"n_estimators\"]\n",
    "print(best_n_estimators)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Being able to quickly find and prioritize the huge volume of information given back from machine learning modeling output is a great skill. Here you had great practice doing that with cv_results_ by quickly isolating the key information on the best performing square. This will be very important when your grids grow from 12 squares to many more!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# See what type of object the best_estimator_ property is\n",
    "print(type(grid_rf_class.best_estimator_))\n",
    "\n",
    "# Create an array of predictions directly using the best_estimator_ property\n",
    "predictions = grid_rf_class.best_estimator_.predict(X_test)\n",
    "\n",
    "# Take a look to confirm it worked, this should be an array of 1's and 0's\n",
    "print(predictions[0:5])\n",
    "\n",
    "# Now create a confusion matrix \n",
    "print(\"Confusion Matrix \\n\", confusion_matrix(y_test, predictions))\n",
    "\n",
    "# Get the ROC-AUC score\n",
    "predictions_proba = grid_rf_class.best_estimator_.predict_proba(X_test)[:,1]\n",
    "print(\"ROC-AUC Score \\n\", roc_auc_score(y_test, predictions_proba))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[0 0 0 0 1]\n",
    "\n",
    "Confusion Matrix \n",
    "\n",
    " [[140   8]\n",
    " \n",
    " [ 36  16]]\n",
    " \n",
    "ROC-AUC Score \n",
    "\n",
    " 0.7436330561330562"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The .best_estimator_ property is a really powerful property to understand for streamlining your machine learning model building process. You now can run a grid search and seamlessly use the best model from that search to make predictions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Random Search"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![random_search](random_search.png)\n",
    "\n",
    "![why_random_search_works](why_random_search_works.png)\n",
    "\n",
    "![probability_trick](probability_trick.png)\n",
    "\n",
    "![probability_trick_missing](probability_trick_missing.png)\n",
    "\n",
    "![probability_trick_getting](probability_trick_getting.png)\n",
    "\n",
    "![probability_trick_advantage](probability_trick_advantage.png)\n",
    "\n",
    "![random_search_important_notes](random_search_important_notes.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Randomly Sample Hyperparameters\n",
    "To undertake a random search, we firstly need to undertake a random sampling of our hyperparameter space.\n",
    "\n",
    "In this exercise, you will firstly create some lists of hyperparameters that can be zipped up to a list of lists. Then you will randomly sample hyperparameter combinations preparation for running a random search."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a list of values for the learning_rate hyperparameter\n",
    "learn_rate_list = list(np.linspace(0.01,1.5,200))\n",
    "\n",
    "# Create a list of values for the min_samples_leaf hyperparameter\n",
    "min_samples_list = list(range(10,41))\n",
    "\n",
    "# Combination list\n",
    "combinations_list = [list(x) for x in product(learn_rate_list, min_samples_list)]\n",
    "\n",
    "# Sample hyperparameter combinations for a random search.\n",
    "random_combinations_index = np.random.choice(range(0, len(combinations_list)), 250, replace=False)\n",
    "combinations_random_chosen = [combinations_list[x] for x in random_combinations_index]\n",
    "\n",
    "# Print the result\n",
    "print(combinations_random_chosen)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You generated some hyperparameter combinations and randomly sampled in that space."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Randomly Search with Random Forest"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Create lists of the values 'gini' and 'entropy' for criterion & \"auto\", \"sqrt\", \"log2\", None for max_features.\n",
    "- Create a list of values between 3 and 55 inclusive for the hyperparameter max_depth and assign to the list max_depth_list. - - Remember that range(N,M) will create a list from N to M-1.\n",
    "- Combine these lists into a list of lists to sample from using product().\n",
    "- Randomly sample 150 models from the combined list and print the result."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create lists for criterion and max_features\n",
    "criterion_list = ['gini', 'entropy']\n",
    "max_feature_list = [\"auto\", \"sqrt\", \"log2\", None]\n",
    "\n",
    "# Create a list of values for the max_depth hyperparameter\n",
    "max_depth_list = list(range(3,56))\n",
    "\n",
    "# Combination list\n",
    "combinations_list = [list(x) for x in product(criterion_list, max_feature_list, max_depth_list)]\n",
    "\n",
    "# Sample hyperparameter combinations for a random search\n",
    "combinations_random_chosen = random.sample(combinations_list, 150)\n",
    "\n",
    "# Print the result\n",
    "print(combinations_random_chosen)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This one was a bit harder but you managed to sample using text options and learned a new function to sample your lists."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualizing a Random Search\n",
    "Visualizing the search space of random search allows you to easily see the coverage of this technique and therefore allows you to see the effect of your sampling on the search space."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample_and_visualize_hyperparameters(n_samples):\n",
    "\n",
    "  # If asking for all combinations, just return the entire list.\n",
    "  if n_samples == len(combinations_list):\n",
    "    combinations_random_chosen = combinations_list\n",
    "  else:\n",
    "    combinations_random_chosen = []\n",
    "    random_combinations_index = np.random.choice(range(0, len(combinations_list)), n_samples, replace=False)\n",
    "    combinations_random_chosen = [combinations_list[x] for x in random_combinations_index]\n",
    "    \n",
    "  # Pull out the X and Y to plot\n",
    "  rand_y, rand_x = [x[0] for x in combinations_random_chosen], [x[1] for x in combinations_random_chosen]\n",
    "\n",
    "  # Plot \n",
    "  plt.clf() \n",
    "  plt.scatter(rand_y, rand_x, c=['blue']*len(combinations_random_chosen))\n",
    "  plt.gca().set(xlabel='learn_rate', ylabel='min_samples_leaf', title='Random Search Hyperparameters')\n",
    "  plt.gca().set_xlim(x_lims)\n",
    "  plt.gca().set_ylim(y_lims)\n",
    "  plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Confirm how many hyperparameter combinations & print\n",
    "number_combs = len(combinations_list)\n",
    "print(number_combs)\n",
    "\n",
    "# Sample and visualise specified combinations\n",
    "for x in [50, 500 , 1500 ]:\n",
    "    sample_and_visualize_hyperparameters(x)\n",
    "    \n",
    "# Sample all the hyperparameter combinations & visualise\n",
    "sample_and_visualize_hyperparameters(number_combs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![50](50.svg)\n",
    "\n",
    "![500](500.svg)\n",
    "\n",
    "![1500](1500.svg)\n",
    "\n",
    "![2000](2000.svg)\n",
    "\n",
    "Those were some great viz you produced! Notice how the bigger your sample space of a random search the more it looks like a grid search?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![random_grid_diff](random_grid_diff.png)\n",
    "\n",
    "![key_diff](key_diff.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The RandomizedSearchCV Object\n",
    "Just like the GridSearchCV library from Scikit Learn, RandomizedSearchCV provides many useful features to assist with efficiently undertaking a random search. You're going to create a RandomizedSearchCV object, making the small adjustment needed from the GridSearchCV object.\n",
    "\n",
    "The desired options are:\n",
    "\n",
    "- A default Gradient Boosting Classifier Estimator\n",
    "- 5-fold cross validation\n",
    "- Use accuracy to score the models\n",
    "- Use 4 cores for processing in parallel\n",
    "- Ensure you refit the best model and return training scores\n",
    "- Randomly sample 10 models\n",
    "- The hyperparameter grid should be for learning_rate (150 values between 0.1 and 2) and min_samples_leaf (all values between and including 20 and 64)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the parameter grid\n",
    "param_grid = {'learning_rate': np.linspace(0.1,2,150), 'min_samples_leaf': list(range(20,65))} \n",
    "\n",
    "# Create a random search object\n",
    "random_GBM_class = RandomizedSearchCV(\n",
    "    estimator = GradientBoostingClassifier(),\n",
    "    param_distributions = param_grid,\n",
    "    n_iter = 10,\n",
    "    scoring='accuracy', n_jobs=4, cv = 5, refit=True, return_train_score = True)\n",
    "\n",
    "# Fit to the training data\n",
    "random_GBM_class.fit(X_train, y_train)\n",
    "\n",
    "# Print the values used for both hyperparameters\n",
    "print(random_GBM_class.cv_results_['param_learning_rate'])\n",
    "print(random_GBM_class.cv_results_['param_min_samples_leaf'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[1.1073825503355705 1.0691275167785235 0.4697986577181208\n",
    " 1.2476510067114095 1.5664429530201343 1.7577181208053692\n",
    " 1.859731543624161 1.5791946308724834 0.5463087248322147\n",
    " 1.7577181208053692]\n",
    "\n",
    "[47 54 61 30 63 32 60 43 38 27]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the parameter grid\n",
    "param_grid = {'max_depth': list(range(5,26)), 'max_features': ['auto' , 'sqrt']} \n",
    "\n",
    "# Create a random search object\n",
    "random_rf_class = RandomizedSearchCV(\n",
    "    estimator = RandomForestClassifier(n_estimators=80),\n",
    "    param_distributions = param_grid, n_iter = 5,\n",
    "    scoring='roc_auc', n_jobs=4, cv = 3, refit=True, return_train_score = True )\n",
    "\n",
    "# Fit to the training data\n",
    "random_rf_class.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[18 11 10 22 10]\n",
    "\n",
    "['sqrt' 'auto' 'sqrt' 'sqrt' 'auto']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Comparing Grid and Random Search\n",
    "\n",
    "![random_grid_similarities](random_grid_similarities.png)\n",
    "\n",
    "![random_grid_differences](random_grid_differences.png)\n",
    "\n",
    "![random_vs_grid](random_vs_grid.png)\n",
    "\n",
    "As you saw, random search tests a larger space of values so is more likely to get close to the best score, given the same computational resources as Grid Search."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Grid and Random Search Side by Side\n",
    "Visualizing the search space of random and grid search together allows you to easily see the coverage that each technique has and therefore brings to life their specific advantages and disadvantages.\n",
    "\n",
    "In this exercise, you will sample hyperparameter combinations in a grid search way as well as a random search way, then plot these to see the difference.\n",
    "\n",
    "You will have available:\n",
    "\n",
    "- combinations_list which is a list of combinations of learn_rate and min_samples_leaf for this algorithm\n",
    "- The function visualize_search() which will make your hyperparameter combinations into X and Y coordinates and plot both grid and random search combinations on the same graph. It takes as input two lists of hyperparameter combinations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sample grid coordinates\n",
    "grid_combinations_chosen = combinations_list[0:300]\n",
    "\n",
    "# Create a list of sample indexes\n",
    "sample_indexes = list(range(0,len(combinations_list)))\n",
    "\n",
    "# Randomly sample 300 indexes\n",
    "random_indexes = np.random.choice(sample_indexes, 300, replace=False)\n",
    "\n",
    "# Use indexes to create random sample\n",
    "random_combinations_chosen = [combinations_list[index] for index in random_indexes]\n",
    "\n",
    "# Call the function to produce the visualization\n",
    "visualize_search(grid_combinations_chosen, random_combinations_chosen)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![grid_random_image](grid_random_image.svg)\n",
    "\n",
    "You can really see how a grid search will cover a small area completely whilst random search will cover a much larger area but not completely."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![uninformed_informed_search](uninformed_informed_search.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Informed Search: Coarse to Fine"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualizing Coarse to Fine\n",
    "\n",
    "You're going to undertake the first part of a Coarse to Fine search. This involves analyzing the results of an initial random search that took place over a large search space, then deciding what would be the next logical step to make your hyperparameter search finer.\n",
    "\n",
    "You have available:\n",
    "\n",
    "- combinations_list - a list of the possible hyperparameter combinations the random search was undertaken on.\n",
    "- results_df - a DataFrame that has each hyperparameter combination and the resulting accuracy of all 500 trials. Each hyperparameter is a column, with the header the hyperparameter name.\n",
    "- visualize_hyperparameter() - a function that takes in a column of the DataFrame (as a string) and produces a scatter plot of this column's values compared to the accuracy scores. An example call of the function would be visualize_hyperparameter('accuracy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Confirm the size of the combinations_list\n",
    "print(len(combinations_list))\n",
    "\n",
    "# Sort the results_df by accuracy and print the top 10 rows\n",
    "print(results_df.sort_values(by='accuracy', ascending=False).head(10))\n",
    "\n",
    "# Confirm which hyperparameters were used in this search\n",
    "print(results_df.columns)\n",
    "\n",
    "# Call visualize_hyperparameter() with each hyperparameter in turn\n",
    "visualize_hyperparameter('max_depth')\n",
    "visualize_hyperparameter('min_samples_leaf')\n",
    "visualize_hyperparameter('learn_rate')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![max_depth](max_depth.svg)\n",
    "\n",
    "![min_samples_leaf](min_samples_leaf.svg)\n",
    "\n",
    "![learn_rate](learn_rate.svg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have undertaken the first step of a Coarse to Fine search. Results clearly seem better when max_depth is below 20. learn_rates smaller than 1 seem to perform well too. There is not a strong trend for min_samples leaf though. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Coarse to Fine Iterations\n",
    "\n",
    "You will now visualize the first random search undertaken, construct a tighter grid and check the results. You will have available:\n",
    "\n",
    "- results_df - a DataFrame that has the hyperparameter combination and the resulting accuracy of all 500 trials. Only the hyperparameters that had the strongest visualizations from the previous exercise are included (max_depth and learn_rate)\n",
    "- visualize_first() - This function takes no arguments but will visualize each of your hyperparameters against accuracy for your first random search."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use the provided function to visualize the first results\n",
    "# visualize_first()\n",
    "\n",
    "# Create some combinations lists & combine:\n",
    "max_depth_list = list(range(1,21))\n",
    "learn_rate_list = np.linspace(0.001,1,50)\n",
    "\n",
    "# Call the function to visualize the second results\n",
    "visualize_second()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![finer_max_depth](finer_max_depth.svg)\n",
    "\n",
    "![finer_learn_rate](finer_learn_rate.svg)\n",
    "\n",
    "You can see in the second example our results are all generally higher. There also appears to be a bump around max_depths between 5 and 10 as well as learn_rate less than 0.2 so perhaps there is even more room for improvement!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Informed Search Bayesian Statistics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![bayes_intro](bayes_intro.png)\n",
    "\n",
    "![bayes_rule](bayes_rule.png)\n",
    "\n",
    "![bayes_rule_ctd](bayes_rule_ctd.png)\n",
    "\n",
    "![bayes_in_medicine](bayes_in_medicine.png)\n",
    "\n",
    "![bayes_in_medicine_ctd](bayes_in_medicine_ctd.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bayes Rule in Python\n",
    "In this exercise you will undertake a practical example of setting up Bayes formula, obtaining new evidence and updating your 'beliefs' in order to get a more accurate result. The example will relate to the likelihood that someone will close their account for your online software product.\n",
    "\n",
    "These are the probabilities we know:\n",
    "\n",
    "- 7% (0.07) of people are likely to close their account next month\n",
    "- 15% (0.15) of people with accounts are unhappy with your product (you don't know who though!)\n",
    "- 35% (0.35) of people who are likely to close their account are unhappy with your product"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assign probabilities to variables \n",
    "p_unhappy = 0.15\n",
    "p_unhappy_close = 0.35\n",
    "\n",
    "# Probabiliy someone will close\n",
    "p_close = 0.07\n",
    "\n",
    "# Probability unhappy person will close\n",
    "p_close_unhappy = (p_unhappy_close * p_close) / p_unhappy\n",
    "print(p_close_unhappy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "0.16333333333333336"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You correctly were able to frame this problem in a Bayesian way, and update your beliefs using new evidence. There's a 16.3% chance that a customer, given that they are unhappy, will close their account."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bayesian Hyperparameter Tuning\n",
    "\n",
    "![bayes_in_hyperparam_tuning](bayes_in_hyperparam_tuning.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bayesian Hyperparameter tuning with Hyperopt\n",
    "![bayesian_hyperparam_tuning_hyperopt](bayesian_hyperparam_tuning_hyperopt.png)\n",
    "In this example you will set up and run a bayesian hyperparameter optimization process using the package Hyperopt (already imported as hp for you). You will set up the domain (which is similar to setting up the grid for a grid search), then set up the objective function. Finally, you will run the optimizer over 20 iterations.\n",
    "\n",
    "You will need to set up the domain using values:\n",
    "\n",
    "- max_depth using quniform distribution (between 2 and 10, increasing by 2)\n",
    "- learning_rate using uniform distribution (0.001 to 0.9)\n",
    "Note that for the purpose of this exercise, this process was reduced in data sample size and hyperopt & GBM iterations. If you are trying out this method by yourself on your own machine, try a larger search space, more trials, more cvs and a larger dataset size to really see this in action!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up space dictionary with specified hyperparameters\n",
    "space = {'max_depth': hp.quniform('max_depth', 2, 10, 2),'learning_rate': hp.uniform('learning_rate', 0.001,0.9)}\n",
    "\n",
    "# Set up objective function\n",
    "def objective(params):\n",
    "    params = {'max_depth': int(params['max_depth']),'learning_rate': params['learning_rate']}\n",
    "    gbm_clf = GradientBoostingClassifier(n_estimators=100, **params) \n",
    "    best_score = cross_val_score(gbm_clf, X_train, y_train, scoring='accuracy', cv=2, n_jobs=4).mean()\n",
    "    loss = 1 - best_score\n",
    "    return loss\n",
    "\n",
    "# Run the algorithm\n",
    "best = fmin(fn=objective,space=space, max_evals=20, rstate=np.random.RandomState(42), algo=tpe.suggest)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "  0%|          | 0/20 [00:00<?, ?it/s, best loss: ?]\n",
    "\n",
    "  5%|5         | 1/20 [00:00<00:13,  1.44it/s, best loss: 0.26759418985474637]\n",
    "\n",
    " 10%|#         | 2/20 [00:01<00:11,  1.53it/s, best loss: 0.2549063726593165] \n",
    "\n",
    " 15%|#5        | 3/20 [00:01<00:10,  1.63it/s, best loss: 0.2549063726593165]\n",
    "\n",
    " 20%|##        | 4/20 [00:02<00:09,  1.63it/s, best loss: 0.2549063726593165]\n",
    "\n",
    " 25%|##5       | 5/20 [00:03<00:12,  1.25it/s, best loss: 0.2549063726593165]\n",
    "\n",
    " 30%|###       | 6/20 [00:04<00:11,  1.20it/s, best loss: 0.2549063726593165]\n",
    "\n",
    " 35%|###5      | 7/20 [00:05<00:09,  1.34it/s, best loss: 0.2549063726593165]\n",
    "\n",
    " 40%|####      | 8/20 [00:05<00:08,  1.37it/s, best loss: 0.2549063726593165]\n",
    "\n",
    " 45%|####5     | 9/20 [00:06<00:07,  1.38it/s, best loss: 0.2549063726593165]\n",
    "\n",
    " 50%|#####     | 10/20 [00:06<00:06,  1.52it/s, best loss: 0.2549063726593165]\n",
    "\n",
    " 55%|#####5    | 11/20 [00:07<00:05,  1.56it/s, best loss: 0.2549063726593165]\n",
    "\n",
    " 60%|######    | 12/20 [00:08<00:05,  1.51it/s, best loss: 0.2549063726593165]\n",
    "\n",
    " 65%|######5   | 13/20 [00:09<00:04,  1.42it/s, best loss: 0.2549063726593165]\n",
    "\n",
    " 70%|#######   | 14/20 [00:10<00:06,  1.04s/it, best loss: 0.2525688142203555]\n",
    "\n",
    " 75%|#######5  | 15/20 [00:11<00:04,  1.12it/s, best loss: 0.2525688142203555]\n",
    "\n",
    " 80%|########  | 16/20 [00:12<00:03,  1.22it/s, best loss: 0.2525688142203555]\n",
    "\n",
    " 85%|########5 | 17/20 [00:13<00:03,  1.06s/it, best loss: 0.24246856171404285]\n",
    "\n",
    " 90%|######### | 18/20 [00:14<00:01,  1.10it/s, best loss: 0.24246856171404285]\n",
    "\n",
    " 95%|#########5| 19/20 [00:14<00:00,  1.22it/s, best loss: 0.24246856171404285]\n",
    "\n",
    "100%|##########| 20/20 [00:15<00:00,  1.34it/s, best loss: 0.24246856171404285]\n",
    "\n",
    "100%|##########| 20/20 [00:15<00:00,  1.29it/s, best loss: 0.24246856171404285]\n",
    "\n",
    "    {'learning_rate': 0.11310589268581149, 'max_depth': 6.0}\n",
    "\n",
    "You succesfully built your first bayesian hyperparameter tuning algorithm. This will be a very powerful tool for your machine learning modeling in future. Bayesian hyperparameter tuning is a new and popular method so this first taster is a valuable thing to gain experience in."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Informed Search: Genetic Algorithms\n",
    "\n",
    "![genetics_evolution_nature](genetics_evolution_nature.png)\n",
    "\n",
    "![genetics_in_ML](genetics_in_ML.png)\n",
    "\n",
    "![genetics_in_ML_advantages](genetics_in_ML_advantages.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TPOT\n",
    "\n",
    "![tpot_intro](tpot_intro.png)\n",
    "\n",
    "![tpot_components](tpot_components.png)\n",
    "\n",
    "![tpot_example](tpot_example.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assign the values outlined to the inputs\n",
    "number_generations = 3\n",
    "population_size = 4\n",
    "offspring_size = 3\n",
    "scoring_function = 'accuracy'\n",
    "\n",
    "# Create the tpot classifier\n",
    "tpot_clf = TPOTClassifier(generations=number_generations, population_size=population_size,\n",
    "                          offspring_size=offspring_size, scoring=scoring_function,\n",
    "                          verbosity=2, random_state=2, cv=2)\n",
    "\n",
    "# Fit the classifier to the training data\n",
    "tpot_clf.fit(X_train, y_train)\n",
    "\n",
    "# Score on the test set\n",
    "print(tpot_clf.score(X_test, y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Warning: xgboost.XGBClassifier is not available and will not be used by TPOT.\n",
    "\n",
    "Generation 1 - Current best internal CV score: 0.7575064376609415\n",
    "\n",
    "Generation 2 - Current best internal CV score: 0.7750693767344183\n",
    "\n",
    "Generation 3 - Current best internal CV score: 0.7750693767344183\n",
    "    \n",
    "Best pipeline: BernoulliNB(input_matrix, alpha=0.1, fit_prior=True)\n",
    "\n",
    "0.76"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can see in the output the score produced by the chosen model (in this case a version of Naive Bayes) over each generation, and then the final accuracy score with the hyperparameters chosen for the final model. This is a great first example of using TPOT for automated hyperparameter tuning. You can now extend on this on your own and build great machine learning models!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Analysing TPOT's stability\n",
    "\n",
    "You will now see the random nature of TPOT by constructing the classifier with different random states and seeing what model is found to be best by the algorithm. This assists to see that TPOT is quite unstable when not run for a reasonable amount of time."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Create the TPOT classifier, fit to the data and score using a random_state of 42.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the tpot classifier \n",
    "tpot_clf = TPOTClassifier(generations=2, population_size=4, offspring_size=3, scoring='accuracy', cv=2,\n",
    "                          verbosity=2, random_state=42)\n",
    "\n",
    "# Fit the classifier to the training data\n",
    "tpot_clf.fit(X_train, y_train)\n",
    "\n",
    "# Score on the test set\n",
    "print(tpot_clf.score(X_test, y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Warning: xgboost.XGBClassifier is not available and will not be used by TPOT.\n",
    "    \n",
    "    Generation 1 - Current best internal CV score: 0.7549688742218555\n",
    "        \n",
    "    Generation 2 - Current best internal CV score: 0.7549688742218555\n",
    "    \n",
    "    Best pipeline: DecisionTreeClassifier(input_matrix, criterion=gini, max_depth=7, min_samples_leaf=11, min_samples_split=12)\n",
    "        \n",
    "    0.75"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Now try using a random_state of 122. The numbers don't mean anything special, but should produce different results.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the tpot classifier \n",
    "tpot_clf = TPOTClassifier(generations=2, population_size=4, offspring_size=3, scoring='accuracy', cv=2,\n",
    "                          verbosity=2, random_state=122)\n",
    "\n",
    "# Fit the classifier to the training data\n",
    "tpot_clf.fit(X_train, y_train)\n",
    "\n",
    "# Score on the test set\n",
    "print(tpot_clf.score(X_test, y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Warning: xgboost.XGBClassifier is not available and will not be used by TPOT.\n",
    "    \n",
    "Generation 1 - Current best internal CV score: 0.7675066876671917\n",
    "    \n",
    "Generation 2 - Current best internal CV score: 0.7675066876671917\n",
    "    \n",
    "Best pipeline: KNeighborsClassifier(MaxAbsScaler(input_matrix), n_neighbors=57, p=1, weights=distance)\n",
    "    \n",
    "0.75"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Finally try using the random_state of 99. See how there is a different result again?**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the tpot classifier \n",
    "tpot_clf = TPOTClassifier(generations=2, population_size=4, offspring_size=3, scoring='accuracy', cv=2,\n",
    "                          verbosity=2, random_state=99)\n",
    "\n",
    "# Fit the classifier to the training data\n",
    "tpot_clf.fit(X_train, y_train)\n",
    "\n",
    "# Score on the test set\n",
    "print(tpot_clf.score(X_test, y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Warning: xgboost.XGBClassifier is not available and will not be used by TPOT.\n",
    "    \n",
    "Generation 1 - Current best internal CV score: 0.8075326883172079\n",
    "    \n",
    "Generation 2 - Current best internal CV score: 0.8075326883172079\n",
    "\n",
    "Best pipeline: RandomForestClassifier(SelectFwe(input_matrix, alpha=0.033), bootstrap=False, criterion=gini, max_features=1.0, min_samples_leaf=19, min_samples_split=10, n_estimators=100)\n",
    "    \n",
    "0.78"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can see that TPOT is quite unstable when only running with low generations, population size and offspring. The first model chosen was a Decision Tree, then a K-nearest Neighbor model and finally a Random Forest. Increasing the generations, population size and offspring and running this for a long time will assist to produce better models and more stable results. Don't hesitate to try it yourself on your own machine!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
